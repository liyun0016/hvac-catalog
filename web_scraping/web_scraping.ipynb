{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping product line: Air Conditioners\n",
      "Scraping product line: Furnaces\n",
      "Scraping product line: Heat Pumps\n",
      "Scraping product line: Fan Coils\n",
      "Scraping product line: Ductless Systems\n",
      "Scraping product line: Boilers\n",
      "Scraping product line: Crossover Solutions\n",
      "Scraping product line: Evaporator Coils\n",
      "Scraping product line: Geothermal Heat Pumps\n",
      "Scraping product line: Packaged Products\n",
      "Scraping product line: Carrier Opti-V\n",
      "Scraping product line: Toshiba Carrier Opti-V\n",
      "Scraping product line: VRF Controls\n",
      "  product_line_name                                 product_line_image  \\\n",
      "0  Air Conditioners  https://images.carriercms.com/image/upload/w_a...   \n",
      "1  Air Conditioners  https://images.carriercms.com/image/upload/w_a...   \n",
      "2  Air Conditioners  https://images.carriercms.com/image/upload/w_a...   \n",
      "3  Air Conditioners  https://images.carriercms.com/image/upload/w_a...   \n",
      "4  Air Conditioners  https://images.carriercms.com/image/upload/w_a...   \n",
      "\n",
      "                                          model_name  \\\n",
      "0  26VNA1 - Infinity®Variable-Speed Central Air C...   \n",
      "1      26TPA8 - Performance™ 2-Stage Air Conditioner   \n",
      "2  26TPA8***C - Performance™ 2-Stage Coastal Air ...   \n",
      "3  26SPA6 - Performance™ Air Conditioner with Int...   \n",
      "4                  26SCA5 - Comfort™ Air Conditioner   \n",
      "\n",
      "                                         model_image  efficiency  \n",
      "0  https://images.carriercms.com/image/upload/w_a...    Up to 21  \n",
      "1  https://images.carriercms.com/image/upload/w_a...    Up to 18  \n",
      "2  https://images.carriercms.com/image/upload/w_a...    Up to 18  \n",
      "3  https://images.carriercms.com/image/upload/w_a...  Up to 16.5  \n",
      "4  https://images.carriercms.com/image/upload/w_a...  Up to 16.5  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings since verify=False is used\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = \"https://www.carrier.com\"\n",
    "START_URL = \"https://www.carrier.com/residential/en/us/products/heating-cooling/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Step 1: Get product line names, image URLs, and detail page links\n",
    "res = requests.get(START_URL, headers=HEADERS, verify=False)\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "product_lines = []\n",
    "\n",
    "for block in soup.select(\"div.col-12.col-md-6.col-xl-4\"):\n",
    "    a_tag = block.find(\"a\", href=True)\n",
    "    img_tag = block.find(\"img\")\n",
    "    name_tag = block.find(\"h3\")\n",
    "\n",
    "    if a_tag and img_tag and name_tag:\n",
    "        line_url = BASE_URL + a_tag[\"href\"].strip()\n",
    "        line_name = name_tag.get_text(strip=True)\n",
    "        line_img = img_tag.get(\"data-src\") or img_tag.get(\"src\")\n",
    "\n",
    "        product_lines.append({\n",
    "            \"product_line_name\": line_name,\n",
    "            \"product_line_url\": line_url,\n",
    "            \"product_line_image\": line_img\n",
    "        })\n",
    "\n",
    "# Step 2: Scrape models from each product line page\n",
    "results = []\n",
    "\n",
    "for line in product_lines:\n",
    "    print(f\"Scraping product line: {line['product_line_name']}\")\n",
    "\n",
    "    try:\n",
    "        res = requests.get(line[\"product_line_url\"], headers=HEADERS, verify=False)\n",
    "        page = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        model_blocks = page.find_all(\"div\", {\"itemtype\": \"http://schema.org/ListItem\"})\n",
    "\n",
    "        for block in model_blocks:\n",
    "            meta_name = block.find(\"meta\", itemprop=\"name\")\n",
    "            title_tag = block.select_one(\"div.card-title a\")\n",
    "            # Get image tag (more flexible)\n",
    "            image_tag = block.select_one(\"img.card-img-top\")\n",
    "\n",
    "            model_image = image_tag[\"data-src\"].strip() if image_tag and image_tag.has_attr(\"data-src\") else (\n",
    "                image_tag[\"src\"].strip() if image_tag and image_tag.has_attr(\"src\") else \"\"\n",
    "            )\n",
    "\n",
    "            if meta_name and title_tag:\n",
    "                name_part = meta_name.get(\"content\", \"\").strip()\n",
    "                title_part = title_tag.get_text(strip=True)\n",
    "                combined_model_name = f\"{name_part} - {title_part}\"\n",
    "\n",
    "                # Get model page URL\n",
    "                model_page_rel_url = title_tag[\"href\"]\n",
    "                model_page_url = BASE_URL + model_page_rel_url\n",
    "\n",
    "                # Step 3: Get COOLING SEER2 value from individual model page\n",
    "                try:\n",
    "                    model_res = requests.get(model_page_url, headers=HEADERS, verify=False)\n",
    "                    model_soup = BeautifulSoup(model_res.content, \"html.parser\")\n",
    "\n",
    "                    seer_label = model_soup.find(\"td\", string=lambda text: text and \"COOLING SEER2\" in text.upper())\n",
    "                    if seer_label and seer_label.find_next_sibling(\"td\"):\n",
    "                        seer_value = seer_label.find_next_sibling(\"td\").get_text(strip=True)\n",
    "                    else:\n",
    "                        seer_value = \"\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching SEER2 from {model_page_url}: {e}\")\n",
    "                    seer_value = \"\"\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    \"product_line_name\": line[\"product_line_name\"],\n",
    "                    \"product_line_image\": line[\"product_line_image\"],\n",
    "                    \"model_name\": combined_model_name,\n",
    "                    \"model_image\": model_image,\n",
    "                    \"efficiency\": seer_value\n",
    "                })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {line['product_line_name']}: {e}\")\n",
    "\n",
    "# Step 4: Save all data to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"carrier_hvac_models.csv\", index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          model_name        efficiency\n",
      "0  26VNA1 - Infinity®Variable-Speed Central Air C...    Up to 21 SEER2\n",
      "1      26TPA8 - Performance™ 2-Stage Air Conditioner    Up to 18 SEER2\n",
      "2  26TPA8***C - Performance™ 2-Stage Coastal Air ...    Up to 18 SEER2\n",
      "3  26SPA6 - Performance™ Air Conditioner with Int...  Up to 16.5 SEER2\n",
      "4                  26SCA5 - Comfort™ Air Conditioner  Up to 16.5 SEER2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"carrier_hvac_models.csv\")\n",
    "\n",
    "# Append ' SEER2' if there's a value\n",
    "df[\"efficiency\"] = df[\"efficiency\"].apply(\n",
    "    lambda x: f\"{x} SEER2\" if pd.notna(x) and str(x).strip() != \"\" else x\n",
    ")\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(\"carrier_hvac_models.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(df[[\"model_name\", \"efficiency\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LENNOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: furnaces\n",
      "Scraping: air-conditioners\n",
      "Scraping: heat-pumps\n",
      "Scraping: packaged-units\n",
      "Scraping: air-handlers\n",
      "Scraping: boilers\n",
      "Scraping: garage-heaters\n",
      "Scraping: mini-split-systems\n",
      "  product_line_name product_line_image  \\\n",
      "0          furnaces                      \n",
      "1          furnaces                      \n",
      "2          furnaces                      \n",
      "3          furnaces                      \n",
      "4          furnaces                      \n",
      "\n",
      "                                          model_name  \\\n",
      "0               SLP99V Variable-Capacity Gas Furnace   \n",
      "1  SL297NV Variable-Speed, Ultra-Low Emissions Ga...   \n",
      "2                  SL280V Variable-Speed Gas Furnace   \n",
      "3  SL280NV Variable-Speed, Ultra-Low Emissions Ga...   \n",
      "4                                 EL297V Gas Furnace   \n",
      "\n",
      "                                         model_image efficiency  \n",
      "0  https://www.lennox.com/dA/4e5724a728/slp99v-pr...   99 SEER2  \n",
      "1  https://www.lennox.com/dA/e90670413a/sl297nv-p...   97 SEER2  \n",
      "2  https://www.lennox.com/dA/4bf421ca4f/sl280v-pr...   80 SEER2  \n",
      "3  https://www.lennox.com/dA/b6b5f684f7/sl280nv-p...   80 SEER2  \n",
      "4  https://www.lennox.com/dA/44f9a033cd/el296v-pr...   97 SEER2  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib3\n",
    "import urllib.parse\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = \"https://www.lennox.com\"\n",
    "START_URL = \"https://www.lennox.com/residential/products/heating-cooling\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Step 1: Get all product lines\n",
    "res = requests.get(START_URL, headers=HEADERS, verify=False)\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "product_lines = []\n",
    "\n",
    "for tile in soup.select(\"div.lnx-container.lnx-grid.products.lnx-grid-three-up a\"):\n",
    "    href = tile.get(\"href\")\n",
    "    if href and href.startswith(\"/residential/products/heating-cooling/\"):\n",
    "        product_line_name = href.strip().split(\"/\")[-1]\n",
    "        product_line_url = BASE_URL + href.strip()\n",
    "        product_lines.append({\n",
    "            \"product_line_name\": product_line_name,\n",
    "            \"product_line_url\": product_line_url\n",
    "        })\n",
    "\n",
    "# Step 2: Scrape models from each product line\n",
    "results = []\n",
    "\n",
    "for line in product_lines:\n",
    "    print(f\"Scraping: {line['product_line_name']}\")\n",
    "    try:\n",
    "        res = requests.get(line[\"product_line_url\"], headers=HEADERS, verify=False)\n",
    "        page = BeautifulSoup(res.content, \"html.parser\")\n",
    "        models = page.select(\"div.lnx-product-tile\")\n",
    "\n",
    "        for m in models:\n",
    "            name_tag = m.select_one(\"h2.lnx-product-title span\")\n",
    "            image_tag = m.select_one(\"div.lnx-product-image img\")\n",
    "            link_tag = m.select_one(\"h2.lnx-product-title a\")\n",
    "\n",
    "            if name_tag and image_tag and link_tag:\n",
    "                model_name = name_tag.get_text(strip=True)\n",
    "                model_image = urllib.parse.urljoin(BASE_URL, image_tag.get(\"src\").strip())\n",
    "                model_url = BASE_URL + link_tag.get(\"href\").strip()\n",
    "\n",
    "                # Step 3: Visit model page and extract SEER2\n",
    "                try:\n",
    "                    model_res = requests.get(model_url, headers=HEADERS, verify=False)\n",
    "                    model_soup = BeautifulSoup(model_res.content, \"html.parser\")\n",
    "\n",
    "                    rating_tag = model_soup.find(\"div\", class_=\"lnx-efficiency-rating\", itemtype=\"http://schema.org/Rating\")\n",
    "                    rating_value_tag = rating_tag.find(\"span\", itemprop=\"ratingValue\") if rating_tag else None\n",
    "                    efficiency = rating_value_tag.get_text(strip=True) + \" SEER2\" if rating_value_tag else \"\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching SEER2 from {model_url}: {e}\")\n",
    "                    efficiency = \"\"\n",
    "\n",
    "                results.append({\n",
    "                    \"product_line_name\": line[\"product_line_name\"],\n",
    "                    \"product_line_image\": \"\",  # Not available\n",
    "                    \"model_name\": model_name,\n",
    "                    \"model_image\": model_image,\n",
    "                    \"efficiency\": efficiency\n",
    "                })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {line['product_line_name']}: {e}\")\n",
    "\n",
    "# Step 4: Save results\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"lennox_hvac_models.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Initialize Selenium Driver\n",
    "# ------------------------------\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # run in background\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Extract Product Lines\n",
    "# ------------------------------\n",
    "def extract_product_lines(driver, start_url, product_line_selector, name_fn, url_fn, image_fn):\n",
    "    driver.get(start_url)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    product_lines = []\n",
    "    for tag in soup.select(product_line_selector):\n",
    "        try:\n",
    "            name = name_fn(tag)\n",
    "            url = url_fn(tag)\n",
    "            image = image_fn(tag)\n",
    "            if name and url:\n",
    "                product_lines.append({\n",
    "                    \"product_line_name\": name,\n",
    "                    \"product_line_url\": url,\n",
    "                    \"product_line_image\": image\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return product_lines\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Extract Models from Line\n",
    "# ------------------------------\n",
    "def extract_models_from_line(driver, product_line, model_selector, name_fn, image_fn, efficiency_fn=None):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for block in soup.select(model_selector):\n",
    "        try:\n",
    "            name = name_fn(block)\n",
    "            image = image_fn(block)\n",
    "            efficiency = efficiency_fn(block) if efficiency_fn else \"\"\n",
    "            if name:\n",
    "                models.append({\n",
    "                    \"model_name\": name,\n",
    "                    \"model_image\": image,\n",
    "                    \"efficiency\": efficiency\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return models\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Scrape Full Brand\n",
    "# ------------------------------\n",
    "def scrape_brand(driver, start_url, line_cfg, model_cfg):\n",
    "    product_lines = extract_product_lines(\n",
    "        driver=driver,\n",
    "        start_url=start_url,\n",
    "        product_line_selector=line_cfg[\"selector\"],\n",
    "        name_fn=line_cfg[\"name_fn\"],\n",
    "        url_fn=line_cfg[\"url_fn\"],\n",
    "        image_fn=line_cfg[\"image_fn\"]\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for line in product_lines:\n",
    "        print(f\"Scraping: {line['product_line_name']}\")\n",
    "        try:\n",
    "            models = extract_models_from_line(\n",
    "                driver=driver,\n",
    "                product_line=line,\n",
    "                model_selector=model_cfg[\"selector\"],\n",
    "                name_fn=model_cfg[\"name_fn\"],\n",
    "                image_fn=model_cfg[\"image_fn\"],\n",
    "                efficiency_fn=model_cfg.get(\"efficiency_fn\")\n",
    "            )\n",
    "            for model in models:\n",
    "                all_results.append({\n",
    "                    \"product_line_name\": line[\"product_line_name\"],\n",
    "                    \"product_line_image\": line[\"product_line_image\"],\n",
    "                    \"model_name\": model[\"model_name\"],\n",
    "                    \"model_image\": model[\"model_image\"],\n",
    "                    \"efficiency\": model.get(\"efficiency\", \"\")\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {line['product_line_name']}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Run Scraper for a Brand\n",
    "# ------------------------------\n",
    "def run_brand_scraper(start_url, line_cfg, model_cfg, output_file):\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        df = scrape_brand(driver, start_url, line_cfg, model_cfg)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Scraping complete. Saved to {output_file}\")\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rheem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Furnaces\n",
      "Scraping: Air Conditioners\n",
      "Scraping: Cooling Coils\n",
      "Scraping: Air Handlers\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Mini-Splits\n",
      "Scraping: Oil Furnaces\n",
      "Scraping complete. Saved to rheem_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "# Rheem-specific CSS selectors and logic\n",
    "rheem_line_cfg = {\n",
    "    \"selector\": \"ul.explorelist a[href]\",\n",
    "    \"name_fn\": lambda tag: tag.get_text(strip=True),\n",
    "    \"url_fn\": lambda tag: tag.get(\"href\").strip(),\n",
    "    \"image_fn\": lambda tag: tag.find(\"img\")[\"src\"].strip() if tag.find(\"img\") else \"\"\n",
    "}\n",
    "\n",
    "rheem_model_cfg = {\n",
    "    \"selector\": \"div.productcard\",\n",
    "    \"name_fn\": lambda tag: tag.select_one(\"div.producttitle h3.product-name\").get_text(strip=True),\n",
    "    \"image_fn\": lambda tag: (\n",
    "        tag.select_one(\"div.productimage img\")[\"srcset\"].split()[0]\n",
    "        if tag.select_one(\"div.productimage img\") and tag.select_one(\"div.productimage img\").has_attr(\"srcset\")\n",
    "        else tag.select_one(\"div.productimage img\")[\"src\"].strip()\n",
    "        if tag.select_one(\"div.productimage img\") and tag.select_one(\"div.productimage img\").has_attr(\"src\")\n",
    "        else \"\"\n",
    "    ),\n",
    "    \"efficiency_fn\": lambda tag: tag.select_one(\"div.product-desc ul li\").get_text(strip=True) if tag.select_one(\"div.product-desc ul li\") else \"\"\n",
    "}\n",
    "\n",
    "# Run for Rheem\n",
    "run_brand_scraper(\n",
    "    start_url=\"https://www.rheem.com/products/residential/heating-and-cooling/\",\n",
    "    line_cfg=rheem_line_cfg,\n",
    "    model_cfg=rheem_model_cfg,\n",
    "    output_file=\"rheem_hvac_models.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## american standard companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Air conditioners\n",
      "Scraping: Furnace\n",
      "Scraping: Heat pumps\n",
      "Scraping: Air handlers\n",
      "Scraping: Coils\n",
      "Scraping: Gas & electric packaged\n",
      "Scraping: Heat pump packaged\n",
      "Scraping: Air conditioner packaged\n",
      "Scraping: Hybrid Dual Fuel Systems\n",
      "Scraping: Single-zone ductless\n",
      "Scraping: Multi-zone ductless\n",
      "Scraping: Ventilation\n",
      "Scraping: Air purification\n",
      "Scraping: Humidity control\n",
      "Scraping: Smart thermostats\n",
      "Scraping: Traditional thermostats\n",
      "Scraping: Zoning\n",
      "Scraping: Light Commercial Products\n",
      "Scraping complete. Saved to american_standard_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "BASE_URL = \"https://www.americanstandardair.com\"\n",
    "\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/products/#heating-cooling\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    product_lines = []\n",
    "    blocks = soup.select(\"div.m-6.inline-block\")\n",
    "\n",
    "    for block in blocks:\n",
    "        a_tag = block.find(\"a\", href=True)\n",
    "        if a_tag and a_tag[\"href\"].startswith(\"/products/\"):\n",
    "            line_name = a_tag.get_text(strip=True)\n",
    "            line_url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            product_lines.append({\n",
    "                \"product_line_name\": line_name,\n",
    "                \"product_line_url\": line_url,\n",
    "                \"product_line_image\": \"\"  # not available at this level\n",
    "            })\n",
    "\n",
    "    return product_lines\n",
    "\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for h3 in soup.select(\"h3.mb-2.text-base\"):\n",
    "        a_tag = h3.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            model_name = a_tag.get_text(strip=True)\n",
    "            model_url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "\n",
    "            # Find image by matching <a> href\n",
    "            model_image = \"\"\n",
    "            img_container = soup.find(\"a\", href=a_tag[\"href\"])\n",
    "            if img_container:\n",
    "                img_tag = img_container.find(\"img\")\n",
    "                if img_tag and img_tag.get(\"src\"):\n",
    "                    model_image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "\n",
    "            efficiency = extract_efficiency(driver, model_url)\n",
    "\n",
    "            models.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "def extract_efficiency(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        wrapper = soup.find(\"div\", class_=\"mb-4 max-w-[40rem] lg:hidden\")\n",
    "        if not wrapper:\n",
    "            return \"\"\n",
    "\n",
    "        grids = wrapper.find_all(\"div\", class_=lambda x: x and x.startswith(\"grid\"))\n",
    "        for grid in grids:\n",
    "            label = grid.find(\"span\", class_=\"ml-3\")\n",
    "            value_div = grid.find(\"div\", class_=\"flex items-center rounded-r-[1rem]\")\n",
    "            if label and value_div and \"SEER2\" in label.get_text(strip=True).upper():\n",
    "                return value_div.get_text(strip=True) + \" SEER2\"\n",
    "\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SEER2 from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def scrape_american_standard():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                for model in models:\n",
    "                    all_results.append({\n",
    "                        \"product_line_name\": line[\"product_line_name\"],\n",
    "                        \"product_line_image\": line[\"product_line_url\"],\n",
    "                        \"model_name\": model[\"model_name\"],\n",
    "                        \"model_image\": model[\"model_image\"],\n",
    "                        \"efficiency\": model[\"efficiency\"]\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models in {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df.to_csv(\"american_standard_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to american_standard_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the scraper\n",
    "scrape_american_standard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Air Conditioners\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Gas Furnaces\n",
      "Scraping: Packaged Units\n",
      "Scraping: Air Handlers and Coils\n",
      "Scraping: Indoor Air Essentials\n",
      "Scraping: Temperature Control\n",
      "Scraping: Ductless Systems\n",
      "Scraping complete. Saved to goodman_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://www.goodmanmfg.com\"\n",
    "\n",
    "# Initialize Selenium driver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/products\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    lines = []\n",
    "    for item in soup.select(\"div.gm-product-item\"):\n",
    "        a_tag = item.find(\"a\", href=True)\n",
    "        img_tag = item.find(\"img\")\n",
    "        name_tag = item.select_one(\"div.gm-category span\")\n",
    "\n",
    "        if a_tag and name_tag:\n",
    "            line_name = name_tag.get_text(strip=True)\n",
    "            line_url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            line_img = img_tag[\"src\"].strip() if img_tag else \"\"\n",
    "            lines.append({\n",
    "                \"product_line_name\": line_name,\n",
    "                \"product_line_url\": line_url,\n",
    "                \"product_line_image\": line_img\n",
    "            })\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Extract models\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for block in soup.select(\"div.gm-product.primary-list\"):\n",
    "        a_tag = block.find(\"a\", href=True, title=True)\n",
    "        img_tag = block.find(\"img\")\n",
    "\n",
    "        if a_tag:\n",
    "            model_name = a_tag.get_text(strip=True)\n",
    "            model_url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            model_image = img_tag[\"src\"].strip() if img_tag else \"\"\n",
    "            efficiency = extract_efficiency(driver, model_url)\n",
    "\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# Extract efficiency\n",
    "def extract_efficiency(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        block = soup.find(\"div\", class_=\"dimension-left\")\n",
    "        if block:\n",
    "            for li in block.find_all(\"li\"):\n",
    "                spans = li.find_all(\"span\")\n",
    "                if len(spans) >= 2 and \"EFFICIENCY\" in spans[0].get_text(strip=True).upper():\n",
    "                    return spans[1].get_text(strip=True)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SEER2 from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Main scraper\n",
    "def scrape_goodman():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models from {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Enforce column order\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "\n",
    "        df.to_csv(\"goodman_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to goodman_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_goodman()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Furnaces\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Mini-Split Systems\n",
      "Scraping: Packaged Units\n",
      "Scraping: Air Conditioners\n",
      "Scraping: Air Handlers\n",
      "Scraping: Evaporator Coils\n",
      "Scraping: Furnaces\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Mini-Split Systems\n",
      "Scraping: Packaged Units\n",
      "Scraping: Air Conditioners\n",
      "Scraping: Air Handlers\n",
      "Scraping: Evaporator Coils\n",
      "Scraping: Furnaces\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Mini-Split Systems\n",
      "Scraping: Packaged Units\n",
      "Scraping: Ultraviolet Air Treatment System\n",
      "Scraping: Whole-House Bypass Humidifier\n",
      "Scraping: Whole-House Dehumidifier\n",
      "Scraping: Whole-House Fan-Powered Humidifier\n",
      "Scraping: Energy Recovery Ventilator\n",
      "Scraping: Hybrid Electronic Air Cleaner\n",
      "Scraping: Media Air Cleaners\n",
      "Scraping: Steam Humidifier\n",
      "Scraping: Ultraviolet Air Treatment System\n",
      "Scraping: Whole-House Bypass Humidifier\n",
      "Scraping: Whole-House Dehumidifier\n",
      "Scraping: Whole-House Fan-Powered Humidifier\n",
      "Scraping: Energy Recovery Ventilator\n",
      "Scraping: Hybrid Electronic Air Cleaner\n",
      "Scraping: Media Air Cleaners\n",
      "Scraping: Steam Humidifier\n",
      "Scraping: Ultraviolet Air Treatment System\n",
      "Scraping: Whole-House Bypass Humidifier\n",
      "Scraping: Whole-House Dehumidifier\n",
      "Scraping: Whole-House Fan-Powered Humidifier\n",
      "Scraping: HX Touch-Screen Thermostat\n",
      "Scraping: HX3 Touch-Screen Thermostat\n",
      "Scraping: HX3 Zoning System\n",
      "Scraping: Universal Thermostat Adapter\n",
      "Scraping complete. Saved to york_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://www.york.com\"\n",
    "\n",
    "# 1. Initialize driver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 2. Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/residential-products#HeatingandCooling\")\n",
    "    time.sleep(4)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    product_lines = []\n",
    "    blocks = soup.select(\"div.productcategories\")\n",
    "\n",
    "    for block in blocks:\n",
    "        a_tag = block.find(\"a\", href=True)\n",
    "        img_tag = block.find(\"img\")\n",
    "        h2_tag = block.find(\"h2\")\n",
    "\n",
    "        if a_tag and img_tag and h2_tag:\n",
    "            name = h2_tag.get_text(strip=True)\n",
    "            url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "            product_lines.append({\n",
    "                \"product_line_name\": name,\n",
    "                \"product_line_url\": url,\n",
    "                \"product_line_image\": image\n",
    "            })\n",
    "\n",
    "    return product_lines\n",
    "\n",
    "# 3. Extract models from product line\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    blocks = soup.select(\"div.col-sm-6.col-md-4\")\n",
    "\n",
    "    for block in blocks:\n",
    "        h2_tag = block.find(\"h2\", class_=\"title\")\n",
    "        img_tag = block.find(\"img\")\n",
    "\n",
    "        if h2_tag and img_tag:\n",
    "            model_name = h2_tag.get_text(strip=True)\n",
    "            model_image = img_tag[\"src\"]\n",
    "            model_image = urllib.parse.urljoin(BASE_URL, model_image)\n",
    "\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": model_name  # per your instruction\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# 4. Main scraper\n",
    "def scrape_york():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models for {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "        df.to_csv(\"york_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to york_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_york()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bryant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Air Conditioners\n",
      "Scraping: Boilers\n",
      "Scraping: Crossover Solutions\n",
      "Scraping: Ductless Systems\n",
      "Scraping: Evaporator Coils\n",
      "Scraping: Fan Coils\n",
      "Scraping: Gas Furnaces\n",
      "Scraping: Geothermal Heat Pumps\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Oil Furnaces\n",
      "Scraping: Packaged Products\n",
      "Scraping: Evolution™ Connex™\n",
      "Scraping: ecobee Smart Thermostats\n",
      "Scraping: Bryant Smart Thermostat\n",
      "Scraping: Air Purifiers\n",
      "Scraping: CO Alarms\n",
      "Scraping: Dehumidifiers\n",
      "Scraping: Humidifiers\n",
      "Scraping: UV Lamps\n",
      "Scraping: Ventilators\n",
      "Scraping: Outdoor Units\n",
      "Scraping: Indoor Units\n",
      "Scraping: Controls & Accessories\n",
      "Scraping: Rooftop\n",
      "Scraping: Split Systems\n",
      "Scraping: About Our Dealers\n",
      "Scraping: Federal Tax Credits\n",
      "Scraping: Financing\n",
      "Scraping: Minimum Efficiency Standards\n",
      "Scraping: Rebates\n",
      "Scraping: Repair or Replace?\n",
      "Scraping: System Types\n",
      "Scraping: Evolution™ System\n",
      "Scraping: Ductless Crossover\n",
      "Scraping: Ductless Systems\n",
      "Scraping: Geothermal Systems\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Indoor Air Quality\n",
      "Scraping: InteliSense Technology\n",
      "Scraping: Puron Advance™\n",
      "Scraping: Smart Thermostats\n",
      "Scraping: Variable Refrigerant Flow (VRF)\n",
      "Scraping: HVAC Parts\n",
      "Scraping: HVAC Filters\n",
      "Scraping: Product Documents\n",
      "Scraping: Register Your Product\n",
      "Scraping: Thermostat & Controls Login\n",
      "Scraping: Servicing Your System\n",
      "Scraping: SmartSave\n",
      "Scraping: SmartHome App\n",
      "Scraping: Troubleshooting\n",
      "Scraping: Warranty Information\n",
      "Scraping: FAQ\n",
      "Scraping: Glossary\n",
      "Scraping: History\n",
      "Scraping: Media Center\n",
      "Scraping: Charles Bryant\n",
      "Scraping: Based In Indianapolis\n",
      "Scraping: Sustainability\n",
      "Scraping: Community Involvement\n",
      "Scraping: Factory Authorized Dealers\n",
      "Scraping: Certified Ductless Pros\n",
      "Scraping: Become a Bryant Dealer\n",
      "Scraping: HVAC Technician Careers\n",
      "Scraping: Contact Us\n",
      "Scraping: Dealer Login\n",
      "Scraping: Find a Dealer\n",
      "Scraping: Air Conditioners\n",
      "Scraping: Ductless Systems\n",
      "Scraping: Gas Furnaces\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Controls & Thermostats\n",
      "Scraping: Federal Tax Credits\n",
      "Scraping: Rebates\n",
      "Scraping: Financing\n",
      "Scraping: Product Documents\n",
      "Scraping: Register Your Product\n",
      "Scraping: Remote Login\n",
      "Scraping: SmartHome App\n",
      "Scraping: Warranty Information\n",
      "Scraping complete. Saved to bryant_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://www.bryant.com\"\n",
    "\n",
    "# 1. Set up Selenium driver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 2. Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/en/us/products/\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    product_lines = []\n",
    "    items = soup.select(\"ul.ct-menusub-basic-header li.nav-item.link-text-indent\")\n",
    "\n",
    "    for li in items:\n",
    "        a_tag = li.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            name = a_tag.get_text(strip=True)\n",
    "            url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            product_lines.append({\n",
    "                \"product_line_name\": name,\n",
    "                \"product_line_url\": url,\n",
    "                \"product_line_image\": \"\"  # no image at product line level\n",
    "            })\n",
    "\n",
    "    return product_lines\n",
    "\n",
    "# 3. Extract models\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    blocks = soup.select(\"div.col-lg-4.col-md-6.pb-4.item-start\")\n",
    "\n",
    "    for block in blocks:\n",
    "        # Model name\n",
    "        name_tag = block.select_one(\"div.card-title a\")\n",
    "        model_name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "\n",
    "        # Model image\n",
    "        img_tag = block.select_one(\"a img\")\n",
    "        model_image = img_tag[\"data-src\"] if img_tag and img_tag.has_attr(\"data-src\") else (\n",
    "            img_tag[\"src\"] if img_tag and img_tag.has_attr(\"src\") else \"\"\n",
    "        )\n",
    "        model_image = urllib.parse.urljoin(BASE_URL, model_image)\n",
    "\n",
    "        # SEER2 efficiency\n",
    "        eff_tag = block.select_one(\"span.COOLING.EFFICIENCY\")\n",
    "        efficiency = eff_tag.get_text(strip=True) if eff_tag else \"\"\n",
    "\n",
    "        if model_name:\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# 4. Main scraper\n",
    "def scrape_bryant():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models from {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "        df.to_csv(\"bryant_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to bryant_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_bryant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Amana Cloud Services\n",
      "Scraping: Air Conditioners\n",
      "Scraping: Gas Furnaces\n",
      "Scraping: Heat Pumps\n",
      "Scraping: Air Handlers and Coils\n",
      "Scraping: Temperature Controls\n",
      "Scraping: Packaged Units\n",
      "Scraping: Indoor Air Essentials\n",
      "Scraping: Ductless Systems\n",
      "Scraping complete. Saved to amana_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://www.amana-hac.com\"\n",
    "\n",
    "# 1. Initialize driver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 2. Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/products\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    lines = []\n",
    "    for block in soup.select(\"div.product\"):\n",
    "        a_tag = block.find(\"a\", href=True)\n",
    "        h3_tag = block.find(\"h3\")\n",
    "        img_tag = block.select_one(\"div.product-img img\")\n",
    "\n",
    "        if a_tag and h3_tag and img_tag:\n",
    "            name = h3_tag.get_text(strip=True)\n",
    "            url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "            img = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "            lines.append({\n",
    "                \"product_line_name\": name,\n",
    "                \"product_line_url\": url,\n",
    "                \"product_line_image\": img\n",
    "            })\n",
    "\n",
    "    return lines\n",
    "\n",
    "# 3. Extract models from product line\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for block in soup.select(\"div.pitem\"):\n",
    "        name_tag = block.select_one(\"div.min-height-box h3\")\n",
    "        img_tag = block.select_one(\"div.product-img img\")\n",
    "        desc = block.select_one(\"div.product-desc\")\n",
    "\n",
    "        # Get model name\n",
    "        model_name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "\n",
    "        # Get image\n",
    "        model_image = img_tag[\"src\"] if img_tag and img_tag.has_attr(\"src\") else \"\"\n",
    "        model_image = urllib.parse.urljoin(BASE_URL, model_image)\n",
    "\n",
    "        # Get SEER2 efficiency\n",
    "        efficiency = \"\"\n",
    "        if desc:\n",
    "            for li in desc.select(\"li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if \"SEER2\" in text:\n",
    "                    efficiency = text\n",
    "                    break\n",
    "\n",
    "        if model_name:\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# 4. Main scraper\n",
    "def scrape_amana():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models from {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "        df.to_csv(\"amana_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to amana_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_amana()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Air conditioners\n",
      "Scraping: Furnaces\n",
      "Scraping: Heat pumps\n",
      "Scraping: Air handlers\n",
      "Scraping: Smart thermostats\n",
      "Scraping: Traditional thermostats\n",
      "Scraping: Zoning\n",
      "Scraping: Gas & electric packaged\n",
      "Scraping: Heat pump packaged\n",
      "Scraping: Earthwise hybrid dual fuel packaged\n",
      "Scraping: Air conditioner packaged\n",
      "Scraping: Single-zone ductless\n",
      "Scraping: Multi-zone ductless\n",
      "Scraping: Filters\n",
      "Scraping: Air purification\n",
      "Scraping: Humidity control\n",
      "Scraping: Ventilation\n",
      "Scraping: Trane Home App\n",
      "Scraping: Coils\n",
      "Scraping complete. Saved to trane_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://www.trane.com\"\n",
    "\n",
    "# 1. Set up Selenium\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 2. Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/residential/en/products/\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    lines = []\n",
    "    for block in soup.select(\"div.border.hover\\\\:shadow-md.p-4.flex.flex-col\"):\n",
    "        name_tag = block.select_one(\"div.min-h-\\\\[140px\\\\] a\")\n",
    "        img_tag = block.select_one(\"a img\")\n",
    "\n",
    "        if name_tag and img_tag:\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            url = urllib.parse.urljoin(BASE_URL, name_tag[\"href\"])\n",
    "            image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "            lines.append({\n",
    "                \"product_line_name\": name,\n",
    "                \"product_line_url\": url,\n",
    "                \"product_line_image\": image\n",
    "            })\n",
    "\n",
    "    return lines\n",
    "\n",
    "# 3. Extract models from product line\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for card in soup.select(\"div.product-card.border.flex.flex-col\"):\n",
    "        model_tag = card.select_one(\"h3.text-base a\")\n",
    "        img_tag = card.select_one(\"a img\")\n",
    "\n",
    "        if model_tag and img_tag:\n",
    "            model_name = model_tag.get_text(strip=True)\n",
    "            model_url = urllib.parse.urljoin(BASE_URL, model_tag[\"href\"])\n",
    "            model_image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "            efficiency = extract_efficiency(driver, model_url)\n",
    "\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# 4. Extract SEER2 efficiency from model detail page\n",
    "def extract_efficiency(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        para = soup.find(\"p\", class_=\"text-white font-medium text-22\")\n",
    "        if para:\n",
    "            text = para.get_text(strip=True)\n",
    "            match = re.search(r\"SEER2\\s+of\\s+up\\s+to\\s+\\d+(\\.\\d+)?\", text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SEER2 from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 5. Run the scraper\n",
    "def scrape_trane():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models from {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "        df.to_csv(\"trane_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to trane_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_trane()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Whole Home Heat Pumps\n",
      "Scraping: Whole Home Air Conditioners\n",
      "Scraping: Air Handlers and Coils\n",
      "Scraping: Gas Furnaces\n",
      "Scraping: Packaged Products\n",
      "Scraping complete. Saved to daikin_hvac_models.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.parse\n",
    "\n",
    "BASE_URL = \"https://daikincomfort.com\"\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Step 1: Extract product lines\n",
    "def extract_product_lines(driver):\n",
    "    driver.get(BASE_URL + \"/products/heating-cooling/whole-home-systems\")\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    lines = []\n",
    "    blocks = soup.select(\"div.row.hoverTab__preview__item\")\n",
    "\n",
    "    for block in blocks:\n",
    "        h6_tag = block.select_one(\"h6\")\n",
    "        img_tag = block.select_one(\"picture img\")\n",
    "        a_tag = block.select_one(\"a.Link[href]\")\n",
    "\n",
    "        if h6_tag and img_tag and a_tag:\n",
    "            name = h6_tag.get_text(strip=True)\n",
    "            image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "            url = urllib.parse.urljoin(BASE_URL, a_tag[\"href\"])\n",
    "\n",
    "            lines.append({\n",
    "                \"product_line_name\": name,\n",
    "                \"product_line_image\": image,\n",
    "                \"product_line_url\": url\n",
    "            })\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Step 2: Extract models\n",
    "def extract_models(driver, product_line):\n",
    "    driver.get(product_line[\"product_line_url\"])\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    models = []\n",
    "    for card in soup.select(\"div.productCard\"):\n",
    "        name_tag = card.select_one(\"h6.productCard__title\")\n",
    "        img_tag = card.select_one(\"a.prodImageLink img\")\n",
    "        eff_block = card.select_one(\"div.productCard__content__info__each\")\n",
    "\n",
    "        model_name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "\n",
    "        model_image = \"\"\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            model_image = urllib.parse.urljoin(BASE_URL, img_tag[\"src\"])\n",
    "\n",
    "        efficiency = \"\"\n",
    "        if eff_block:\n",
    "            label = eff_block.find(\"h6\")\n",
    "            value = eff_block.find(\"p\")\n",
    "            if label and value and \"SEER2\" in label.get_text(strip=True).upper():\n",
    "                efficiency = value.get_text(strip=True) + \" \" + label.get_text(strip=True)\n",
    "\n",
    "        if model_name:\n",
    "            models.append({\n",
    "                \"product_line_name\": product_line[\"product_line_name\"],\n",
    "                \"product_line_image\": product_line[\"product_line_image\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"model_image\": model_image,\n",
    "                \"efficiency\": efficiency\n",
    "            })\n",
    "\n",
    "    return models\n",
    "\n",
    "# Step 3: Run full scraper\n",
    "def scrape_daikin():\n",
    "    driver = init_driver()\n",
    "    try:\n",
    "        product_lines = extract_product_lines(driver)\n",
    "        all_results = []\n",
    "\n",
    "        for line in product_lines:\n",
    "            print(f\"Scraping: {line['product_line_name']}\")\n",
    "            try:\n",
    "                models = extract_models(driver, line)\n",
    "                all_results.extend(models)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping models from {line['product_line_name']}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df[[\"product_line_name\", \"product_line_image\", \"model_name\", \"model_image\", \"efficiency\"]]\n",
    "        df.to_csv(\"daikin_hvac_models.csv\", index=False)\n",
    "        print(\"Scraping complete. Saved to daikin_hvac_models.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "scrape_daikin()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
